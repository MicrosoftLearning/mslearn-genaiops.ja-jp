---
lab:
  title: 生成 AI アプリケーションを監視する
  description: デプロイされたモデルとの対話を監視し、生成 AI アプリケーションでの使用を最適化する方法に関する分析情報を取得する方法について学習します。
---

# 生成 AI アプリケーションを監視する

この演習は約 **30** 分かかります。

> **注**: この演習では、Azure AI Foundry に関するある程度の知識を前提としているため、よりアクティブな探索と実践的な学習を促すために、一部の手順は意図的にあまり詳細に説明されていません。

## はじめに

この演習では、チャット補完アプリの監視を有効にして、そのパフォーマンスを Azure Monitor で確認します。 デプロイされたモデルとやり取りしてデータを生成し、生成されたデータを生成 AI アプリケーションの分析情報ダッシュボードで確認し、モデルのデプロイの最適化に役立つアラートを設定します。

## 環境を設定する

この演習のタスクを完了するには、以下が必要です。

- Azure AI Foundry プロジェクト
- デプロイされたモデル (GPT-4o など)
- 接続された Application Insights リソース

### Azure AI Foundry プロジェクトにモデルをデプロイする

Azure AI Foundry プロジェクトをすばやくセットアップするため、Azure AI Foundry ポータル UI を使用する簡単な手順を以下に示します。

1. Web ブラウザーで [Azure AI Foundry ポータル](https://ai.azure.com) (`https://ai.azure.com`) を開き、Azure 資格情報を使用してサインインします。
1. ホーム ページの **[モデルと機能を調査する]** セクションで、プロジェクトで使用する `gpt-4o` モデルを検索します。
1. 検索結果で **gpt-4o** モデルを選んで詳細を確認してから、モデルのページの上部にある **[このモデルを使用する]** を選択します。
1. プロジェクトの作成を求められたら、プロジェクトの有効な名前を入力し、**[詳細]** オプションを展開します。
1. **[カスタマイズ]** を選択し、プロジェクトに次の設定を指定します。
    - **Azure AI Foundry リソース**: *Azure AI Foundry リソースの有効な名前*
    - **[サブスクリプション]**:"*ご自身の Azure サブスクリプション*"
    - **リソース グループ**: *リソース グループを作成または選択します*
    - **リージョン**: ***AI サービスでサポートされている場所を選択します***\*

    > \* 一部の Azure AI リソースは、リージョンのモデル クォータによって制限されます。 演習の後半でクォータ制限を超えた場合は、別のリージョンに別のリソースを作成する必要が生じる可能性があります。

1. **[作成]** を選択し、選んだ gpt-4 モデル デプロイを含むプロジェクトが作成されるまで待ちます。
1. 左側のナビゲーション ウィンドウで **[概要]** を選択すると、プロジェクトのメイン ページが表示されます。
1. **[エンドポイントとキー]** の領域で、**[Azure AI Foundry]** ライブラリが選択されていることを確認し、**[Azure AI Foundry プロジェクト エンドポイント]** を表示します。
1. エンドポイントをメモ帳に**保存**します。 クライアント アプリケーションで、このエンドポイントを使用してプロジェクトに接続します。

### Application Insights の接続

Application Insights を Azure AI Foundry のプロジェクトに接続して、監視するデータの収集を開始します。

1. 左側のメニューを使用し、**[トレース]** ページを選択します。
1. アプリに接続する Application Insights リソースを**新規作成**します。
1. Application Insights リソース名を入力し、**[作成]** を選択します。

Application Insights がプロジェクトに接続され、分析のためにデータの収集が開始されます。

## デプロイ済みモデルとやり取りする

Azure Cloud Shell で Azure AI Foundry プロジェクトへの接続をセットアップして、デプロイしたモデルをプログラムで操作します。 これにより、モデルにプロンプトを送信し、監視データを生成できます。

### Cloud Shell を使用してモデルと接続する

まず、モデルとやり取りするための認証に必要な情報を取得します。 次に、Azure Cloud Shell にアクセスし、指定されたプロンプトを独自のデプロイ済みモデルに送信するように構成を更新します。

1. 新しいブラウザー タブを開きます (既存のタブで Azure AI Foundry ポータルを開いたままにします)。
1. 新しいブラウザー タブで [Azure portal](https://portal.azure.com) (`https://portal.azure.com`) を参照し、メッセージに応じて Azure 資格情報を使用してサインインします。
1. ページ上部の検索バーの右側にある **[\>_]** ボタンを使用して、Azure portal に新しい Cloud Shell を作成し、サブスクリプションにストレージがない ***PowerShell*** 環境を選択します。
1. Cloud Shell ツール バーの **[設定]** メニューで、**[クラシック バージョンに移動]** を選択します。

    **<font color="red">続行する前に、クラシック バージョンの Cloud Shell に切り替えたことを確認します。</font>**

1. Cloud Shell ペインで、次のコマンドを入力して実行します。

    ```
    rm -r mslearn-genaiops -f
    git clone https://github.com/microsoftlearning/mslearn-genaiops mslearn-genaiops
    ```

    このコマンドは、この演習のコード ファイルを含んだ GitHub リポジトリを複製します。

    > **ヒント**: Cloudshell にコマンドを貼り付けると、出力が大量のスクリーン バッファーを占有する可能性があります。 `cls` コマンドを入力して、各タスクに集中しやすくすることで、スクリーンをクリアできます。

1. リポジトリが複製されたら、アプリケーション コード ファイルを含むフォルダーに移動します。  

    ```
   cd mslearn-genaiops/Files/07
    ```

1. Cloud Shell コマンド ライン ペインで、次のコマンドを入力して、必要なライブラリをインストールします。

    ```
   python -m venv labenv
   ./labenv/bin/Activate.ps1
   pip install python-dotenv openai azure-identity azure-ai-projects azure-ai-inference azure-monitor-opentelemetry
    ```

1. 次のコマンドを入力して、提供されている構成ファイルを開きます。

    ```
   code .env
    ```

    このファイルをコード エディターで開きます。

1. コード ファイルで以下を行います。

    1. コード ファイルで、**[your_project_endpoint]** プレースホルダーをプロジェクトのエンドポイント (Azure AI Foundry ポータルでプロジェクトの **[概要]** ページからコピーしたもの) に置き換えます。
    1. **your_model_deployment** プレースホルダーを、GPT-4o モデル デプロイに割り当てた名前 (既定では `gpt-4o`) に置き換えます。

1. プレースホルダーを置き換えた "後"、コード エディター内で、**CTRL + S** コマンドまたは**右クリック > [保存]** を使用して**変更を保存**し、**CTRL + Q** コマンドまたは**右クリック > [終了]** を使用して、Cloud Shell コマンド ラインを開いたままコード エディターを閉じます。**

### デプロイ済みモデルにプロンプトを送信する

次に、デプロイ済みモデルに異なるプロンプトを送信する複数のスクリプトを実行します。 これらのやり取りにより、後で Azure Monitor で確認できるデータが生成されます。

1. 次のコマンドを実行して、提供されている**最初のスクリプトを表示**します。

    ```
   code start-prompt.py
    ```

1. Cloud Shell コマンド ライン ペインで、次のコマンドを入力してアプリを実行します。

    ```
   az login
    ```

    **<font color="red">Cloud Shell セッションが既に認証されている場合でも、Azure にサインインする必要があります。</font>**

    > **注**: ほとんどのシナリオでは、*az ログイン*を使用するだけで十分です。 ただし、複数のテナントにサブスクリプションがある場合は、*[--tenant]* パラメーターを使用してテナントを指定する必要があります。 詳細については、「[Azure CLI を使用して対話形式で Azure にサインインする](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)」を参照してください。
    
1. メッセージが表示されたら、指示に従って新しいタブでサインイン ページを開き、指定された認証コードと Azure 資格情報を入力します。 次に、コマンド ラインでサインイン プロセスを完了し、プロンプトが表示されたら、Azure AI Foundry ハブを含むサブスクリプションを選択します。
1. サインインしたら、次のコマンドを入力してアプリケーションを実行します。

    ```
   python start-prompt.py
    ```

    応答がモデルによって生成され、さらに分析するために Application Insights でキャプチャされます。 その効果を調べるために、プロンプトを変えてみましょう。

1. **スクリプトを開いて確認します**。ここでは、プロンプトはモデルに対して、**1 つの文と一覧でのみ回答する**ように指示しています。

    ```
   code short-prompt.py
    ```

1. コマンド ラインで次のコマンドを入力して、**スクリプトを実行**します。

    ```
   python short-prompt.py
    ```

1. 次のスクリプトの目的も同様ですが、ユーザー メッセージではなく**システム メッセージ**での出力を求める指示が含まれています。

    ```
   code system-prompt.py
    ```

1. コマンド ラインで次のコマンドを入力して、**スクリプトを実行**します。

    ```
   python system-prompt.py
    ```

1. 最後に、**多すぎるトークン**を含むプロンプトを実行して、エラーをトリガーしてみましょう。

    ```
   code error-prompt.py
    ```

1. コマンド ラインで次のコマンドを入力して、**スクリプトを実行**します。 **エラーが発生する可能性が非常に高い**ことに注意してください。

    ```
   python error-prompt.py
    ```

これで、モデルとやり取りしたので、Azure Monitor でデータを確認できます。

> **注**: 監視データが Azure Monitor に表示されるまでに数分かかる場合があります。

## Azure Monitor で監視データを表示する

モデルとのやり取りの結果収集されたデータを表示するには、Azure Monitor で、ブックにリンクしているダッシュボードにアクセスします。

### Azure AI Foundry ポータルから Azure Monitor に移動する

1. **Azure AI Foundry ポータル**が開いている、ブラウザーのタブに移動します。
1. 左側のメニューを使用し、**[トレース]** を選択します。
1. 上部にある、**生成 AI アプリケーションに関する分析情報ダッシュボードを確認する**ためのリンクを選択します。 Azure Monitor が新しいタブで開きます。
1. **[概要]** で、デプロイ済みモデルとのやり取りの概要データを確認します。

## Azure Monitor で監視メトリックを解釈する

次は、データを詳細に調べて、何を意味しているかを解釈していきます。

### トークンの使用状況を確認する

まず **[トークンの使用状況]** セクションに注目し、次のメトリックを確認します。

- **プロンプト トークン**: モデル呼び出し全体を通して入力 (送信したプロンプト) で使用されたトークンの合計数。

> これは、モデルに*質問するコスト*と考えてください。

- **完了トークン**: モデルが出力として返したトークンの数 (基本的には応答の長さ)。

> 生成された完了トークンは、多くの場合、トークンの使用量とコストの大部分を表します。長い回答や詳細な回答の場合は、特にそうです。

- **合計トークン**: プロンプト トークンと完了トークンを合算したもの。

> 待ち時間とコストの増加につながるので、課金とパフォーマンスの面で最も重要なメトリックです。

- **呼び出しの合計数**: 個別の推論要求の数。これは、モデルが呼び出された回数です。

> スループットを分析し、呼び出しあたりの平均コストを把握するのに役立ちます。

### 個々のプロンプトを比較する

下にスクロールして、**[生成 AI のスパン]** を見つけます。これは、各プロンプトが新しいデータ行として表されるテーブルとして視覚化されます。 次の列の内容を確認し比較します。

- **状態**: モデル呼び出しが成功したか失敗したかを示します。

> これを使用すると、問題のあるプロンプトや構成エラーを特定できます。 最後のプロンプトは、長すぎたために失敗した可能性があります。

- **期間**: モデルの応答にかかった時間 (ミリ秒単位) を示します。

> 行全体を比較して、結果的に処理時間が長くなるプロンプト パターンを調べます。

- **入力**: モデルに送信されたユーザー メッセージを表示します。

> この列を使用すると、どのプロンプト定式化が効率的かまたは問題があるかを評価できます。

- **システム**: プロンプトで使用されているシステム メッセージを表示します (存在する場合)。

> エントリを比較すると、システム メッセージの使用または変更の影響を評価できます。

- **出力**: モデルの応答を含んでいます。

> これを使用すると、詳細度、関連度、一貫性を評価できます。 特に、トークンの数と期間に関連します。

## (省略可能) アラートを作成する

時間的余裕があれば、モデルの待ち時間が特定のしきい値を超えたときに通知するアラートを設定してみてください。 これは、チャレンジ課題となるように設計された演習です。つまり、手順は意図的にあまり詳細に説明されていません。

- Azure Monitor で、Azure AI Foundry プロジェクトとモデルの**新しいアラート ルール**を作成します。
- **要求期間 (ミリ秒)** などのメトリックを選択し、しきい値 (たとえば 4000 ミリ秒など) を定義します。
- **新しいアクション グループ**を作成して、通知方法を定義します。

アラートは、プロアクティブな監視を確立することになるので、運用環境の準備に役立ちます。 構成するアラートは、プロジェクトの優先順位と、リスクの測定および軽減方法をチームがどのように決定したかによって異なります。

## 他のラボの参照先

その他のラボと演習については、[Azure AI Foundry ラーニング ポータル](https://ai.azure.com)で調べてください。また、他の利用可能なアクティビティについては、このコースの**ラボ セクション**をご覧ください。
